# ğŸ›¡ï¸ Network Threat Detection using Machine Learning (Phishing Website Classification)

## ğŸ” Project Overview

This project aims to detect phishing websites using machine learning classification models trained on a network security dataset. It follows a robust **end-to-end MLOps + DevOps pipeline** that includes:

âœ… **ETL data pipelines** for ingestion, validation, and transformation  
âœ… **Model training, evaluation**, and version tracking using **MLflow**  
âœ… **Deployment of predictions** through a **FastAPI** web application  
âœ… **Data ingestion and storage** via **MongoDB Atlas (Cloud DB)**  
âœ… **Full CI/CD automation** using **GitHub Actions**  
âœ… **DevOps workflows** for containerization, orchestration, and deployment using **Docker**, **Amazon ECR**, and **EC2**

This integration of both **MLOps** (machine learning lifecycle) and **DevOps** (build â†’ test â†’ deploy â†’ monitor) practices ensures that the solution is **scalable, reproducible, and production-ready**.

---

## ğŸš€ Key Features

### ğŸ§  MLOps Capabilities

- âœ… **End-to-End ML Pipeline**: Ingestion, Validation, Transformation, Model Training  
- âœ… **Experiment Tracking** with **MLflow** (hosted on **DagsHub**)  
- âœ… **Modular architecture** with reusable components (`config` / `entity` based structure)  
- âœ… **MongoDB Atlas** for scalable and cloud-based data ingestion  
- âœ… **Auto-logging** of performance metrics (**Accuracy**, **F1**, **Precision**, **Recall**)  

---

### ğŸŒ Web Application

- âœ… **FastAPI-based REST API** for real-time prediction  
- âœ… **HTML Table rendering** of predictions using **Jinja2 templates**  
- âœ… **Swagger documentation** via `/docs` route for testing the API  

---

### ğŸ› ï¸ DevOps & CI/CD

- âœ… **Dockerized Application** for cross-platform portability  
- âœ… **Secure Image Build & Push** to **Amazon Elastic Container Registry (ECR)**  
- âœ… **Automatic Deployment to AWS EC2 Instance** using SSH via **GitHub Actions**  
- âœ… **GitHub Secrets** integration for sensitive credentials  
- âœ… **Infrastructure commands** to bootstrap EC2 instance with Docker runtime  
- âœ… **Branch-based GitHub workflow** for CI/CD pipeline  

---

## ğŸ§  Problem Statement

Phishing attacks are one of the most common and dangerous cyber threats today. This project builds a machine learning-based classification system that detects whether a URL is **ğŸŸ¢ Safe** or **ğŸ”´ Malicious (Threat)**.

---

## âš™ï¸ Tech Stack

| Layer              | Tech Used                                     |
|-------------------|-----------------------------------------------|
| Language           | Python 3                                      |
| Web App            | FastAPI + Jinja2 Templates                    |
| ML Tracking        | MLflow + DagsHub                              |
| Data Storage       | MongoDB Atlas                                 |
| Deployment         | Docker + Amazon ECR + EC2                     |
| CI/CD              | GitHub Actions                                |
| Visualization      | HTML Table with Styled Predictions            |

---

## ğŸ—ï¸ Project Architecture

```
ğŸ“¦ Network-Phishing-Detection  
â”‚
â”œâ”€â”€ .github/workflows/                      # GitHub Actions for CI/CD
â”‚   â””â”€â”€ main.yml                            # CI/CD pipeline to automate test/build/deploy
â”‚
â”œâ”€â”€ data_schema/                            # Contains schema definitions for input data
â”‚   â””â”€â”€ schema.yaml                         # Defines expected input data format and types
â”‚
â”œâ”€â”€ final_model/                            # Stores the final model and preprocessor
â”‚   â”œâ”€â”€ model.pkl                           # Trained ML model
â”‚   â””â”€â”€ preprocessor.pkl                    # Fitted preprocessing pipeline (e.g., scaler/encoder)
â”‚
â”œâ”€â”€ Network_Data/                           # Raw or intermediate data used for training
â”‚   â””â”€â”€ phisingData.csv                     # Original phishing dataset
â”‚
â”œâ”€â”€ networksecurity/                        # Main project package
â”‚   â”œâ”€â”€ cloud/                              # For cloud logic like S3, GCS etc.
â”‚   â”‚   â””â”€â”€ s3_syncer.py                    # (Optional) to sync artifacts to/from cloud
â”‚
â”‚   â”œâ”€â”€ components/                         # Core ETL pipeline components
â”‚   â”‚   â”œâ”€â”€ data_ingestion.py               # Handles data loading from MongoDB/local sources
â”‚   â”‚   â”œâ”€â”€ data_transformation.py          # Preprocessing, feature engineering logic
â”‚   â”‚   â”œâ”€â”€ data_validation.py              # Validates schema, nulls, and structure
â”‚   â”‚   â””â”€â”€ model_trainer.py                # Contains training logic and evaluation
â”‚
â”‚   â”œâ”€â”€ constant/                           # Constants used across the pipeline
â”‚   â”‚   â””â”€â”€ training_pipeline/              # Training-related constant definitions
â”‚
â”‚   â”œâ”€â”€ entity/                             # Data classes for config and artifact objects
â”‚   â”‚   â”œâ”€â”€ artifact_entity.py              # Defines output artifacts for each pipeline stage
â”‚   â”‚   â””â”€â”€ config_entity.py                # Holds configuration data for components
â”‚
â”‚   â”œâ”€â”€ exception/                          # Centralized exception handling
â”‚   â”‚   â””â”€â”€ exception.py                    # Custom exception class with traceback logging
â”‚
â”‚   â”œâ”€â”€ logging/                            # Central logging utility
â”‚   â”‚   â””â”€â”€ logger.py                       # Custom logger used across the project
â”‚
â”‚   â”œâ”€â”€ pipeline/                           # Manages the overall pipeline execution
â”‚   â”‚   â””â”€â”€ training_pipeline.py            # Orchestrates the complete training pipeline
â”‚
â”‚   â”œâ”€â”€ utils/                              # Common utilities for reusability
â”‚   â”‚   â”œâ”€â”€ main_utils/
â”‚   â”‚   â”‚   â””â”€â”€ utils.py                    # General utilities (file I/O, Mongo helpers, etc.)
â”‚   â”‚   â””â”€â”€ ml_utils/                       # ML-specific utilities
â”‚   â”‚       â”œâ”€â”€ metric/
â”‚   â”‚       â”‚   â””â”€â”€ classification_metrics.py  # Classification performance metrics
â”‚   â”‚       â””â”€â”€ model/
â”‚   â”‚           â””â”€â”€ estimator.py            # Custom estimator wrapper class
â”‚
â”œâ”€â”€ prediction_output/                      # Stores model prediction results
â”‚   â””â”€â”€ output.csv                          # Output file with predicted results
â”‚
â”œâ”€â”€ templates/                              # HTML templates for FastAPI
â”‚   â””â”€â”€ table.html                          # Displays predictions in a styled table
â”‚
â”œâ”€â”€ valid_data/                             # Validated test datasets
â”‚   â””â”€â”€ test.csv                            # Test file post-validation
â”‚
â”œâ”€â”€ app.py                                  # FastAPI app to serve predictions via REST API
â”œâ”€â”€ main.py                                 # Script to run the training pipeline end-to-end
â”œâ”€â”€ push_data.py                            # Script to push local data to MongoDB Atlas
â”œâ”€â”€ Dockerfile                              # Docker container definition
â”œâ”€â”€ .env                                    # Stores environment variables and secrets (ignored in .gitignore)
â”œâ”€â”€ requirements.txt                        # Python packages needed to run the project
â”œâ”€â”€ setup.py                                # Makes the project pip-installable
â””â”€â”€ README.md                               # You're here! Full documentation and project overview
```

---

## ğŸ§ª ML Pipeline (ETL)

| Stage              | Description |
|-------------------|-------------|
| `Data Ingestion`  | Loads raw data, pushes to MongoDB, reads from it |
| `Validation`      | Validates schema, checks nulls etc.              |
| `Transformation`  | Applies encoding, imputation, scaling            |
| `Model Trainer`   | Trains multiple models, performs grid search     |
| `Evaluation`      | Uses `accuracy`, `F1`, `Precision`, `Recall`     |
| `Prediction`      | Outputs `Safe` / `Threat` label (with emoji ğŸŸ¢/ğŸ”´) |

All experiments are logged to **MLflow via DagsHub**.

---

## ğŸŒ Web Application (FastAPI)

A lightweight and styled FastAPI app to:
- âœ… Trigger model training
- âœ… Upload `.csv` file for predictions
- âœ… See styled prediction output in tabular form

ğŸ”— Visit `/docs` for Swagger UI.  
ğŸ” Visit `/view-prediction` to visually inspect predictions.

---

## ğŸŒ MongoDB Atlas Integration

1. **Push Data from Local**:
   ```bash
   python push_data.py
   ```

2. **Read from MongoDB Atlas**:  
   Automatically handled in data ingestion using:

   ```python
   client = pymongo.MongoClient(MONGO_DB_URL, tlsCAFile=certifi.where())
   ```

---

## ğŸ³ Docker Setup

To build and run locally:

```bash
# Build image
docker build -t Phishing-Detector-MLOps-DevOps .

# Run container
docker run -p 8000:8000 Phishing-Detector-MLOps-DevOps
```

---

## ğŸš€ AWS Deployment (ECR + EC2)

### ğŸ” GitHub Secrets

Set these secrets in your GitHub repository:

```env
AWS_ACCESS_KEY_ID=
AWS_SECRET_ACCESS_KEY=
AWS_REGION=
AWS_ECR_LOGIN_URI=
ECR_REPOSITORY_NAME=
```

### ğŸ’¡ EC2 Setup (One-Time)

Run the following on a fresh EC2 instance:

```bash
# Optional
sudo apt update -y && sudo apt upgrade -y

# Docker Installation
curl -fsSL https://get.docker.com -o get-docker.sh
sudo sh get-docker.sh

# Add current user to Docker group
sudo usermod -aG docker ubuntu
newgrp docker
```

---

## ğŸ› ï¸ GitHub Actions

Workflow file: `.github/workflows/main.yml`

It performs:

- âœ… Builds Docker Image  
- âœ… Pushes Image to Amazon ECR  
- âœ… SSH into EC2  
- âœ… Pulls Image on EC2  
- âœ… Runs Docker Container  

---

## ğŸ” CI/CD Flow

```vbnet
Push to Main Branch
    â†“
GitHub Actions Triggered
    â†“
Build Docker Image
    â†“
Push to Amazon ECR
    â†“
SSH into EC2
    â†“
Pull & Run Docker Container
    â†“
App Live on Port 8080! ğŸš€
```

---

## ğŸ§ª How to Run Locally

### ğŸ Setup Virtual Environment

```bash
# 1. Create the virtual environment named `mlops_env`
conda create -n mlops_env python=3.10 -y

# 2. Activate the environment (on Linux/Mac)
conda activate mlops_env

# Install dependencies
pip install -r requirements.txt
```

---

## ğŸ§  Train the Model

```bash
python main.py
```

---

## ğŸŒ Run the Web App

```bash
uvicorn app:app --reload
```

### Visit:

- http://localhost:8000/docs  
- http://localhost:8000/view-prediction

---

## ğŸ“ˆ MLflow + DagsHub

Integrated with DagsHub for model tracking.  
No extra steps needed â€” all metrics are automatically logged via:

```python
dagshub.init(
    repo_owner="Mehul-kh2005", 
    repo_name="Phishing-Detector-MLOps-DevOps", 
    mlflow=True
)
```

---

## ğŸ™‹ Author

### Mehul Khandelwal - ğŸ”— [GitHub](https://github.com/Mehul-kh2005/Phishing-Detector-MLOps-DevOps) | [LinkedIn](https://www.linkedin.com/in/mehulkhandelwal2005/)

ğŸ“ *This project was developed as part of a machine learning, MLOps, and DevOps exercise focused on classification modeling for detecting phishing websites. It integrates a complete ETL pipeline, model training, a FastAPI-based web application, containerization with Docker, and CI/CD deployment to AWS using GitHub Actions â€” delivering a real-world, production-ready implementation of modern data science and DevOps workflows.*